{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Soft Action Critic"
      ],
      "metadata": {
        "id": "Sggv8mqAZJVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "KQat8lGHZQQo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnmsnafIJaC9",
        "outputId": "e000f0a9-a167-4ed3-a40e-68c825f575eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.22.4)\n",
            "Collecting jax-jumpy>=1.0.0 (from gymnasium)\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, jax-jumpy, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.22.4)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.3 (from gymnasium[box2d])\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.1.1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2812123 sha256=7235f566b84f9556478d3029550ed556f2782a39d609b49028462f97743437d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py, pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.3.0\n",
            "    Uninstalling pygame-2.3.0:\n",
            "      Successfully uninstalled pygame-2.3.0\n",
            "Successfully installed box2d-py-2.3.5 pygame-2.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install -q swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "BFqh8SYWLzt4",
        "outputId": "abaed343-8548-4539-87b1-62ce0fc41e46"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make('LunarLanderContinuous-v2', render_mode=\"rgb_array\")\n",
        "env.reset()\n",
        "\n",
        "plt.imshow(env.render())\n",
        "plt.axis(\"off\");"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definitions"
      ],
      "metadata": {
        "id": "XmvVUqF4aqCE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "et8n6C7KLkEz"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "02c3TxOzLpBP"
      },
      "outputs": [],
      "source": [
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "def create_gym_environment(name):\n",
        "  environment = gym.make(name, render_mode=\"rgb_array\")\n",
        "  environment = RecordVideo(environment,\n",
        "                            video_folder=f\"./{name}_recored_episodes\",\n",
        "                            episode_trigger=lambda x: x % 25 == 0,\n",
        "                            disable_logger=True)\n",
        "\n",
        "  return environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TPfON8xTMPvs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import Sequential, Linear, ReLU, Module\n",
        "\n",
        "class QNet(Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, out_dims):\n",
        "    super().__init__()\n",
        "    self.net = Sequential(\n",
        "        Linear(obs_size + out_dims, hidden_size),\n",
        "        ReLU(),\n",
        "        Linear(hidden_size, hidden_size),\n",
        "        ReLU(),\n",
        "        Linear(hidden_size, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, state, action):\n",
        "    in_vector = torch.hstack((state, action)).float()\n",
        "    return self.net(in_vector)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Vehco9SwMzIy"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "from torch.nn import Sequential, Linear, ReLU, Module\n",
        "from torch.nn.functional import softplus\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class Policy(Module):\n",
        "  def __init__(self, hidden_size, obs_size, out_dims):\n",
        "    super().__init__()\n",
        "\n",
        "    self.net = Sequential(\n",
        "        Linear(obs_size, hidden_size),\n",
        "        ReLU(),\n",
        "        Linear(hidden_size, hidden_size),\n",
        "        ReLU()\n",
        "    )\n",
        "\n",
        "    self.linear_mu = Linear(hidden_size, out_dims)\n",
        "    self.linear_std = Linear(hidden_size, out_dims)\n",
        "\n",
        "  def forward(self, obs):\n",
        "      x = self.net(obs)\n",
        "      mu = self.linear_mu(x)\n",
        "      std = self.linear_std(x)\n",
        "      std = softplus(std) + 1e-3\n",
        "\n",
        "      dist = Normal(mu, std)\n",
        "      action = dist.rsample()\n",
        "\n",
        "      log_prob = dist.log_prob(action)\n",
        "      log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
        "\n",
        "      action = torch.tanh(action)\n",
        "\n",
        "      return action, log_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5vWNSwL5nCdG"
      },
      "outputs": [],
      "source": [
        "def polyak_average(net, target_net, tau=0.01):\n",
        "  for qp, tp in zip(net.parameters(), target_net.parameters()):\n",
        "    tp.data.copy_(tau * qp.data + (1 - tau) * tp.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XawHpgp1SPBZ"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "from torch.nn.functional import smooth_l1_loss\n",
        "from torch.optim import AdamW\n",
        "import itertools\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class SoftActionCritic():\n",
        "\n",
        "  def __init__(self, env_name, q_nets = None, schedulers=None, capacity=100_000,\n",
        "               batch_size=256, lr=1e-3, hidden_size=128, gamma=0.99, alpha=0.02, tau=0.05,\n",
        "               loss_fn=smooth_l1_loss, optim=AdamW, eps_start=1.0, eps_end=0.15,\n",
        "               eps_last_episode=600, samples_per_epoch=1024, sync_rate=25, repeat_action_rate=5):\n",
        "      self.env_name = env_name\n",
        "      self.env = create_gym_environment(env_name)\n",
        "\n",
        "      obs_size = self.env.observation_space.shape[0]\n",
        "      n_actions = self.env.action_space.shape[0]\n",
        "      if q_nets:\n",
        "        self.q_net1 = q_nets[\"q_net1\"].to(device)\n",
        "        self.q_net2 = q_nets[\"q_net2\"].to(device)\n",
        "        self.policy = q_nets[\"policy\"].to(device)\n",
        "      else:\n",
        "        self.q_net1 = QNet(hidden_size, obs_size, n_actions).to(device)\n",
        "        self.q_net2 = QNet(hidden_size, obs_size, n_actions).to(device)\n",
        "        self.policy = Policy(hidden_size, obs_size, n_actions).to(device)\n",
        "\n",
        "      self.target_q_net1 = deepcopy(self.q_net1)\n",
        "      self.target_q_net2 = deepcopy(self.q_net2)\n",
        "      self.target_policy = deepcopy(self.policy)\n",
        "\n",
        "      self.schedulers = schedulers\n",
        "\n",
        "      self.buffer = ReplayBuffer(capacity=capacity)\n",
        "      self.loss_fn = loss_fn\n",
        "\n",
        "      self.gamma = gamma\n",
        "      self.alpha = alpha\n",
        "      self.tau = tau\n",
        "      self.batch_size = batch_size\n",
        "      self.eps_start = eps_start\n",
        "      self.eps_end = eps_end\n",
        "      self.eps_last_episode = eps_last_episode\n",
        "      self.sync_rate = sync_rate\n",
        "      self.samples_per_epoch = samples_per_epoch\n",
        "      self.lr = lr\n",
        "\n",
        "      q_net_params = itertools.chain(self.q_net1.parameters(), self.q_net2.parameters())\n",
        "      self.q_net_optimizer = optim(q_net_params, lr=self.lr)\n",
        "      self.policy_optimizer = optim(self.policy.parameters(), lr=self.lr)\n",
        "\n",
        "      self.current_epoch = 1\n",
        "      self.log = []\n",
        "      self.returns = []\n",
        "      self.episode_lengths = []\n",
        "      self.start_time = time.time()\n",
        "      self.repeat_action_rate = repeat_action_rate\n",
        "\n",
        "      while len(self.buffer) < self.samples_per_epoch:\n",
        "        self.play_episode()\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def play_episode(self, policy=None):\n",
        "      state = self.env.reset()[0]\n",
        "      state = torch.tensor(state, device=device)\n",
        "      done = False\n",
        "      rewards = 0\n",
        "      epsiode_length = 0\n",
        "\n",
        "      while not done:\n",
        "\n",
        "        if policy:\n",
        "          action, _ = self.policy(state)\n",
        "          action = action.cpu().numpy()\n",
        "        else:\n",
        "          action = self.env.action_space.sample()\n",
        "\n",
        "        for _ in range(self.repeat_action_rate):\n",
        "          next_state, reward, done1, done2, info = self.env.step(action)\n",
        "          rewards += reward\n",
        "          epsiode_length += 1\n",
        "        done = done1 or done2\n",
        "\n",
        "        next_state = torch.tensor(next_state, device=device)\n",
        "        action = torch.tensor(action, device=device)\n",
        "        reward = torch.tensor(reward, device=device)\n",
        "        done = torch.tensor(done, device=device)\n",
        "        exp = (state, action, reward, done, next_state)\n",
        "\n",
        "        self.buffer.append(exp)\n",
        "        state = next_state\n",
        "      return rewards, epsiode_length\n",
        "\n",
        "  def training_step(self):\n",
        "      batch_T = self.buffer.sample(self.batch_size)\n",
        "      batch = list(map(torch.stack, zip(*batch_T)))\n",
        "\n",
        "      states, actions, rewards, dones, next_states = batch\n",
        "      rewards = rewards.unsqueeze(1)\n",
        "      dones = dones.unsqueeze(1)\n",
        "\n",
        "      action_values1 = self.q_net1(states, actions)\n",
        "      action_values2 = self.q_net2(states, actions)\n",
        "\n",
        "      target_actions, target_log_probs = self.target_policy(next_states)\n",
        "\n",
        "      next_action_values = torch.min(\n",
        "          self.target_q_net1(next_states, target_actions),\n",
        "          self.target_q_net2(next_states, target_actions)\n",
        "      )\n",
        "\n",
        "      next_action_values[dones] = 0.0\n",
        "\n",
        "      expected_action_values = rewards + self.gamma * (next_action_values - self.alpha*target_log_probs)\n",
        "\n",
        "      q_loss1 = self.loss_fn(action_values1, expected_action_values)\n",
        "      q_loss2 = self.loss_fn(action_values2, expected_action_values)\n",
        "\n",
        "      q_loss_total = q_loss1 + q_loss2\n",
        "\n",
        "      self.q_net_optimizer.zero_grad()\n",
        "      q_loss_total.backward()\n",
        "      self.q_net_optimizer.step()\n",
        "\n",
        "\n",
        "      actions, log_probs = self.policy(states)\n",
        "\n",
        "      action_values = torch.min(self.q_net1(states, actions),\n",
        "                                self.q_net2(states, actions)\n",
        "                                )\n",
        "\n",
        "      policy_loss = (self.alpha * log_probs - action_values).mean()\n",
        "\n",
        "      self.policy_optimizer.zero_grad()\n",
        "      policy_loss.backward()\n",
        "      self.policy_optimizer.step()\n",
        "\n",
        "      return q_loss_total, policy_loss\n",
        "\n",
        "  def step_schedulers(self):\n",
        "    if self.schedulers:\n",
        "      for scheduler in self.schedulers:\n",
        "        scheduler(self)\n",
        "\n",
        "  def training_epoch_end(self):\n",
        "    last_return, episode_length = self.play_episode(self.policy)\n",
        "\n",
        "    polyak_average(self.q_net1, self.target_q_net1, tau=self.tau)\n",
        "    polyak_average(self.q_net2, self.target_q_net2, tau=self.tau)\n",
        "    polyak_average(self.policy, self.target_policy, tau=self.tau)\n",
        "\n",
        "    self.step_schedulers()\n",
        "    self.current_epoch += 1\n",
        "    return last_return, episode_length\n",
        "\n",
        "\n",
        "  def fit(self, n_epoch):\n",
        "    for epoch in range(n_epoch):\n",
        "      loss_total = 0\n",
        "      for _ in range(self.samples_per_epoch//self.batch_size):\n",
        "        loss = self.training_step()\n",
        "\n",
        "      last_return, episode_length  = self.training_epoch_end()\n",
        "\n",
        "      self.returns.append(last_return)\n",
        "      self.episode_lengths.append(episode_length)\n",
        "      self.log.append([self.current_epoch, last_return])\n",
        "\n",
        "      if self.current_epoch % 25 == 0:\n",
        "        print(f\"Epoch: {self.current_epoch}, mean return: {np.mean(self.returns[-10:]):.2f}, \" \\\n",
        "          f\"mean episode length: {np.mean(self.episode_lengths[-10:])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "-5hNegr6asex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def alpha_scheduler(algo):\n",
        "  epoch = algo.current_epoch\n",
        "  previous_alpha = algo.alpha\n",
        "  if epoch > 1000:\n",
        "    algo.alpha = 0.01\n",
        "  elif epoch > 600:\n",
        "   algo.alpha = 0.02\n",
        "  elif epoch > 400:\n",
        "   algo.alpha = 0.03\n",
        "  elif epoch > 200:\n",
        "    algo.alpha = 0.04\n",
        "  if previous_alpha != algo.alpha:\n",
        "    print(f\"Current \\\"alpha\\\" is {algo.alpha}\")"
      ],
      "metadata": {
        "id": "Ky3VkLsoxTIN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(\"/content/LunarLanderContinuous-v2_recored_episodes\", ignore_errors=True)\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "NCBwNBVAaksx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alg = SoftActionCritic('LunarLanderContinuous-v2', q_nets=None, schedulers=[alpha_scheduler], repeat_action_rate=3)\n",
        "alg.fit(2000)"
      ],
      "metadata": {
        "id": "d7uvw-VGky40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(alg.policy, \"SAC_LunarLander_policy\")\n",
        "torch.save(alg.q_net1, \"SAC_LunarLander_qnet1\")\n",
        "torch.save(alg.q_net2, \"SAC_LunarLander_qnet2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vweaP3DPtP4U",
        "outputId": "49b087ec-d293-4b2b-9efb-5db572dd93e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "KmlGOe6KtA7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = torch.load(\"SAC_LunarLander_policy\")"
      ],
      "metadata": {
        "id": "Z4USZDtMtvPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "def create_test_gym_environment(name):\n",
        "  environment = gym.make(name, render_mode=\"rgb_array\")\n",
        "  environment = RecordVideo(environment, video_folder=f\"./test_{name}\", episode_trigger=lambda x: x % 1 == 0)\n",
        "\n",
        "  return environment"
      ],
      "metadata": {
        "id": "lAfPxQ2MoOiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def play_episode(test_env, policy, repeat_action_rate):\n",
        "    state = test_env.reset()[0]\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        state_v = torch.tensor(state)\n",
        "        action, _ = policy(state_v)\n",
        "        action = action.cpu().numpy()\n",
        "\n",
        "        for _ in range(repeat_action_rate):\n",
        "          next_state, reward, done1, done2, info = test_env.step(action)\n",
        "        done = done1 or done2\n",
        "        state = next_state"
      ],
      "metadata": {
        "id": "FgH__yNpo7jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_env = create_test_gym_environment('LunarLanderContinuous-v2')\n",
        "\n",
        "for _ in range(10):\n",
        "  play_episode(test_env, policy, 2)"
      ],
      "metadata": {
        "id": "DK6r0wF_X3KJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
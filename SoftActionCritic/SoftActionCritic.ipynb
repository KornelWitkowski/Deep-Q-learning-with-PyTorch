{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Soft Action Critic"
      ],
      "metadata": {
        "id": "Sggv8mqAZJVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "KQat8lGHZQQo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnmsnafIJaC9",
        "outputId": "84ba86a9-621b-4a45-9838-28ec9f71359c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.22.4)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.22.4)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.1.3)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install -q swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "BFqh8SYWLzt4",
        "outputId": "696c9c9c-c972-4141-b53c-02f6ff03fdab"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make('LunarLanderContinuous-v2', render_mode=\"rgb_array\")\n",
        "env.reset()\n",
        "\n",
        "plt.imshow(env.render())\n",
        "plt.axis(\"off\");"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definitions"
      ],
      "metadata": {
        "id": "XmvVUqF4aqCE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "et8n6C7KLkEz"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "02c3TxOzLpBP"
      },
      "outputs": [],
      "source": [
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "def create_gym_environment(name):\n",
        "  environment = gym.make(name, render_mode=\"rgb_array\")\n",
        "  environment = RecordVideo(environment, video_folder=f\"./{name}_recored_episodes\", episode_trigger=lambda x: x % 10 == 0, disable_logger=True)\n",
        "\n",
        "  return environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "TPfON8xTMPvs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import Sequential, Linear, ReLU, Module\n",
        "\n",
        "class DQN(Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, out_dims):\n",
        "    super().__init__()\n",
        "    self.net = Sequential(\n",
        "        Linear(obs_size + out_dims, hidden_size),\n",
        "        ReLU(),\n",
        "        Linear(hidden_size, hidden_size),\n",
        "        ReLU(),\n",
        "        Linear(hidden_size, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, state, action):\n",
        "    in_vector = torch.hstack((state, action)).float()\n",
        "    return self.net(in_vector)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "Vehco9SwMzIy"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "from torch.nn import Sequential, Linear, ReLU, Module\n",
        "from torch.nn.functional import softplus\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "\n",
        "class GradientPolicy(Module):\n",
        "  def __init__(self, hidden_size, obs_size, out_dims):\n",
        "    super().__init__()\n",
        "\n",
        "    self.net = Sequential(\n",
        "        Linear(obs_size, hidden_size),\n",
        "        ReLU(),\n",
        "        Linear(hidden_size, hidden_size),\n",
        "        ReLU()\n",
        "    )\n",
        "\n",
        "    self.linear_mu = Linear(hidden_size, out_dims)\n",
        "    self.linear_std = Linear(hidden_size, out_dims)\n",
        "\n",
        "  def forward(self, obs):\n",
        "      x = self.net(obs)\n",
        "      mu = self.linear_mu(x)\n",
        "      std = self.linear_std(x)\n",
        "      std = softplus(std) + 1e-3\n",
        "\n",
        "      dist = Normal(mu, std)\n",
        "      action = dist.rsample()\n",
        "\n",
        "      log_prob = dist.log_prob(action)\n",
        "      log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
        "\n",
        "      action = torch.tanh(action)\n",
        "\n",
        "      return action, log_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "5vWNSwL5nCdG"
      },
      "outputs": [],
      "source": [
        "def polyak_average(net, target_net, tau=0.01):\n",
        "  for qp, tp in zip(net.parameters(), target_net.parameters()):\n",
        "    tp.data.copy_(tau * qp.data + (1 - tau) * tp.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "XawHpgp1SPBZ"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "from torch.nn.functional import smooth_l1_loss\n",
        "from torch.optim import AdamW\n",
        "import itertools\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class SoftActionCritic():\n",
        "\n",
        "  def __init__(self, env_name, q_nets = None, schedulers=None, capacity=100_000,\n",
        "               batch_size=256, lr=1e-3, hidden_size=128, gamma=0.99, alpha=0.02, tau=0.05,\n",
        "               loss_fn=smooth_l1_loss, optim=AdamW, eps_start=1.0, eps_end=0.15,\n",
        "               eps_last_episode=600, samples_per_epoch=1024, sync_rate=25, repeat_action_rate=5):\n",
        "      self.env_name = env_name\n",
        "      self.env = create_gym_environment(env_name)\n",
        "\n",
        "      obs_size = self.env.observation_space.shape[0]\n",
        "      n_actions = self.env.action_space.shape[0]\n",
        "      if q_nets:\n",
        "        self.q_net1 = q_nets[\"q_net1\"]\n",
        "        self.q_net2 = q_nets[\"q_net2\"]\n",
        "        self.policy = q_nets[\"policy\"]\n",
        "      else:\n",
        "        self.q_net1 = DQN(hidden_size, obs_size, n_actions)\n",
        "        self.q_net2 = DQN(hidden_size, obs_size, n_actions)\n",
        "        self.policy = GradientPolicy(hidden_size, obs_size, n_actions)\n",
        "\n",
        "      self.target_q_net1 = deepcopy(self.q_net1)\n",
        "      self.target_q_net2 = deepcopy(self.q_net2)\n",
        "      self.target_policy = deepcopy(self.policy)\n",
        "\n",
        "      self.schedulers = schedulers\n",
        "\n",
        "      self.buffer = ReplayBuffer(capacity=capacity)\n",
        "      self.loss_fn = loss_fn\n",
        "\n",
        "      self.gamma = gamma\n",
        "      self.alpha = alpha\n",
        "      self.tau = tau\n",
        "      self.batch_size = batch_size\n",
        "      self.eps_start = eps_start\n",
        "      self.eps_end = eps_end\n",
        "      self.eps_last_episode = eps_last_episode\n",
        "      self.sync_rate = sync_rate\n",
        "      self.samples_per_epoch = samples_per_epoch\n",
        "      self.lr = lr\n",
        "\n",
        "      q_net_params = itertools.chain(self.q_net1.parameters(), self.q_net2.parameters())\n",
        "      self.q_net_optimizer = optim(q_net_params, lr=self.lr)\n",
        "      self.policy_optimizer = optim(self.policy.parameters(), lr=self.lr)\n",
        "\n",
        "      self.current_epoch = 1\n",
        "      self.log = []\n",
        "      self.returns = []\n",
        "      self.episode_lengths = []\n",
        "      self.start_time = time.time()\n",
        "      self.repeat_action_rate = repeat_action_rate\n",
        "\n",
        "      while len(self.buffer) < self.samples_per_epoch:\n",
        "        self.play_episode()\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def play_episode(self, policy=None):\n",
        "      state = self.env.reset()[0]\n",
        "      done = False\n",
        "      rewards = 0\n",
        "      epsiode_length = 0\n",
        "\n",
        "      while not done:\n",
        "        state_v = torch.tensor(state)\n",
        "        if policy:\n",
        "          action, _ = self.policy(state_v)\n",
        "          action = action.cpu().numpy()\n",
        "        else:\n",
        "          action = self.env.action_space.sample()\n",
        "\n",
        "        for _ in range(self.repeat_action_rate):\n",
        "          next_state, reward, done1, done2, info = self.env.step(action)\n",
        "          rewards += reward\n",
        "          epsiode_length += 1\n",
        "        done = done1 or done2\n",
        "\n",
        "        next_state_v = torch.tensor(next_state)\n",
        "        action_v = torch.tensor(action)\n",
        "        reward_v = torch.tensor(reward)\n",
        "        done_v = torch.tensor(done)\n",
        "        exp = (state_v, action_v, reward_v, done_v, next_state_v)\n",
        "\n",
        "        self.buffer.append(exp)\n",
        "        state = next_state\n",
        "      return rewards, epsiode_length\n",
        "\n",
        "  def training_step(self):\n",
        "      batch_T = self.buffer.sample(self.batch_size)\n",
        "      batch = list(map(torch.stack, zip(*batch_T)))\n",
        "\n",
        "      states, actions, rewards, dones, next_states = batch\n",
        "      rewards = rewards.unsqueeze(1)\n",
        "      dones = dones.unsqueeze(1)\n",
        "\n",
        "      action_values1 = self.q_net1(states, actions)\n",
        "      action_values2 = self.q_net2(states, actions)\n",
        "\n",
        "      target_actions, target_log_probs = self.target_policy(next_states)\n",
        "\n",
        "      next_action_values = torch.min(\n",
        "          self.target_q_net1(next_states, target_actions),\n",
        "          self.target_q_net2(next_states, target_actions)\n",
        "      )\n",
        "\n",
        "      next_action_values[dones] = 0.0\n",
        "\n",
        "      expected_action_values = rewards + self.gamma * (next_action_values - self.alpha*target_log_probs)\n",
        "\n",
        "      q_loss1 = self.loss_fn(action_values1, expected_action_values)\n",
        "      q_loss2 = self.loss_fn(action_values2, expected_action_values)\n",
        "\n",
        "      q_loss_total = q_loss1 + q_loss2\n",
        "\n",
        "      self.q_net_optimizer.zero_grad()\n",
        "      q_loss_total.backward()\n",
        "      self.q_net_optimizer.step()\n",
        "\n",
        "\n",
        "      actions, log_probs = self.policy(states)\n",
        "\n",
        "      action_values = torch.min(self.q_net1(states, actions),\n",
        "                                self.q_net2(states, actions)\n",
        "                                )\n",
        "\n",
        "      policy_loss = (self.alpha * log_probs - action_values).mean()\n",
        "\n",
        "      self.policy_optimizer.zero_grad()\n",
        "      policy_loss.backward()\n",
        "      self.policy_optimizer.step()\n",
        "\n",
        "      return q_loss_total, policy_loss\n",
        "\n",
        "  def step_schedulers(self):\n",
        "    if self.schedulers:\n",
        "      for scheduler in self.schedulers:\n",
        "        scheduler(self)\n",
        "\n",
        "  def training_epoch_end(self):\n",
        "    last_return, episode_length = self.play_episode(self.policy)\n",
        "\n",
        "    polyak_average(self.q_net1, self.target_q_net1, tau=self.tau)\n",
        "    polyak_average(self.q_net2, self.target_q_net2, tau=self.tau)\n",
        "    polyak_average(self.policy, self.target_policy, tau=self.tau)\n",
        "\n",
        "    self.step_schedulers()\n",
        "    self.current_epoch += 1\n",
        "    return last_return, episode_length\n",
        "\n",
        "\n",
        "  def fit(self, n_epoch):\n",
        "    for epoch in range(n_epoch):\n",
        "      loss_total = 0\n",
        "      for _ in range(self.samples_per_epoch//self.batch_size):\n",
        "        loss = self.training_step()\n",
        "\n",
        "      last_return, episode_length  = self.training_epoch_end()\n",
        "\n",
        "      self.returns.append(last_return)\n",
        "      self.episode_lengths.append(episode_length)\n",
        "      self.log.append([self.current_epoch, last_return])\n",
        "\n",
        "      if self.current_epoch % 25 == 0:\n",
        "        print(f\"Epoch: {self.current_epoch}, mean return: {np.mean(self.returns[-10:]):.2f}, \" \\\n",
        "          f\"mean episode length: {np.mean(self.episode_lengths[-10:])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "-5hNegr6asex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def alpha_scheduler(algo):\n",
        "  epoch = algo.current_epoch\n",
        "  previous_alpha = algo.alpha\n",
        "  if epoch > 1000:\n",
        "    algo.alpha = 0.01\n",
        "  elif epoch > 600:\n",
        "   algo.alpha = 0.02\n",
        "  elif epoch > 400:\n",
        "   algo.alpha = 0.03\n",
        "  elif epoch > 200:\n",
        "    algo.alpha = 0.04\n",
        "  if previous_alpha != algo.alpha:\n",
        "    print(f\"Current \\\"alpha\\\" is {algo.alpha}\")"
      ],
      "metadata": {
        "id": "Ky3VkLsoxTIN"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(\"/content/LunarLanderContinuous-v2_recored_episodes\", ignore_errors=True)\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "NCBwNBVAaksx"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alg = SoftActionCritic('LunarLanderContinuous-v2', q_nets=None, schedulers=[alpha_scheduler], repeat_action_rate=2)\n",
        "alg.fit(2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7uvw-VGky40",
        "outputId": "8b4fac97-70d0-4bb4-c2b2-2115948fc346"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 25, mean return: -311.62, mean episode length: 113.2\n",
            "Epoch: 50, mean return: -127.07, mean episode length: 675.8\n",
            "Epoch: 75, mean return: -273.08, mean episode length: 696.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 100, mean return: -119.60, mean episode length: 901.2\n",
            "Epoch: 125, mean return: -106.94, mean episode length: 906.0\n",
            "Epoch: 150, mean return: -51.89, mean episode length: 977.8\n",
            "Epoch: 175, mean return: -77.57, mean episode length: 992.2\n",
            "Epoch: 200, mean return: -80.85, mean episode length: 1000.0\n",
            "Current \"alpha\" is 0.04\n",
            "Epoch: 225, mean return: -82.83, mean episode length: 972.8\n",
            "Epoch: 250, mean return: -40.62, mean episode length: 953.4\n",
            "Epoch: 275, mean return: -47.54, mean episode length: 947.8\n",
            "Epoch: 300, mean return: -48.78, mean episode length: 947.4\n",
            "Epoch: 325, mean return: 16.33, mean episode length: 944.2\n",
            "Epoch: 350, mean return: 13.33, mean episode length: 938.8\n",
            "Epoch: 375, mean return: -18.95, mean episode length: 638.0\n",
            "Epoch: 400, mean return: -91.35, mean episode length: 632.4\n",
            "Current \"alpha\" is 0.03\n",
            "Epoch: 425, mean return: -71.09, mean episode length: 624.4\n",
            "Epoch: 450, mean return: -117.94, mean episode length: 338.0\n",
            "Epoch: 475, mean return: -93.24, mean episode length: 418.6\n",
            "Epoch: 500, mean return: -113.02, mean episode length: 415.4\n",
            "Epoch: 525, mean return: -100.87, mean episode length: 501.4\n",
            "Epoch: 550, mean return: -94.88, mean episode length: 454.0\n",
            "Epoch: 575, mean return: -136.36, mean episode length: 376.2\n",
            "Epoch: 600, mean return: -84.34, mean episode length: 555.6\n",
            "Current \"alpha\" is 0.02\n",
            "Epoch: 625, mean return: -83.72, mean episode length: 678.6\n",
            "Epoch: 650, mean return: -119.44, mean episode length: 653.6\n",
            "Epoch: 675, mean return: -170.07, mean episode length: 419.6\n",
            "Epoch: 700, mean return: -130.86, mean episode length: 436.0\n",
            "Epoch: 725, mean return: -74.40, mean episode length: 715.0\n",
            "Epoch: 750, mean return: 16.96, mean episode length: 736.0\n",
            "Epoch: 775, mean return: -101.73, mean episode length: 420.2\n",
            "Epoch: 800, mean return: -54.26, mean episode length: 584.0\n",
            "Epoch: 825, mean return: 16.36, mean episode length: 852.4\n",
            "Epoch: 850, mean return: -97.86, mean episode length: 321.2\n",
            "Epoch: 875, mean return: -26.40, mean episode length: 628.8\n",
            "Epoch: 900, mean return: -53.61, mean episode length: 805.0\n",
            "Epoch: 925, mean return: -37.66, mean episode length: 571.2\n",
            "Epoch: 950, mean return: -37.42, mean episode length: 585.0\n",
            "Epoch: 975, mean return: -75.29, mean episode length: 626.8\n",
            "Epoch: 1000, mean return: -63.73, mean episode length: 632.8\n",
            "Current \"alpha\" is 0.01\n",
            "Epoch: 1025, mean return: -89.74, mean episode length: 481.6\n",
            "Epoch: 1050, mean return: -49.57, mean episode length: 557.8\n",
            "Epoch: 1075, mean return: 12.73, mean episode length: 619.0\n",
            "Epoch: 1100, mean return: 17.21, mean episode length: 712.0\n",
            "Epoch: 1125, mean return: -11.53, mean episode length: 744.0\n",
            "Epoch: 1150, mean return: -27.67, mean episode length: 690.8\n",
            "Epoch: 1175, mean return: 94.98, mean episode length: 827.2\n",
            "Epoch: 1200, mean return: 6.10, mean episode length: 660.4\n",
            "Epoch: 1225, mean return: -29.38, mean episode length: 811.8\n",
            "Epoch: 1250, mean return: -53.01, mean episode length: 600.2\n",
            "Epoch: 1275, mean return: -41.20, mean episode length: 692.0\n",
            "Epoch: 1300, mean return: -24.45, mean episode length: 685.4\n",
            "Epoch: 1325, mean return: 49.23, mean episode length: 653.4\n",
            "Epoch: 1350, mean return: 69.85, mean episode length: 566.6\n",
            "Epoch: 1375, mean return: 18.40, mean episode length: 549.2\n",
            "Epoch: 1400, mean return: 45.56, mean episode length: 796.0\n",
            "Epoch: 1425, mean return: -96.91, mean episode length: 510.0\n",
            "Epoch: 1450, mean return: -5.73, mean episode length: 680.8\n",
            "Epoch: 1475, mean return: 46.60, mean episode length: 503.4\n",
            "Epoch: 1500, mean return: -12.74, mean episode length: 795.6\n",
            "Epoch: 1525, mean return: -10.85, mean episode length: 784.6\n",
            "Epoch: 1550, mean return: 76.74, mean episode length: 718.2\n",
            "Epoch: 1575, mean return: 82.18, mean episode length: 554.2\n",
            "Epoch: 1600, mean return: 36.63, mean episode length: 633.6\n",
            "Epoch: 1625, mean return: 61.91, mean episode length: 687.4\n",
            "Epoch: 1650, mean return: 117.55, mean episode length: 712.8\n",
            "Epoch: 1675, mean return: 5.40, mean episode length: 574.0\n",
            "Epoch: 1700, mean return: 168.32, mean episode length: 804.8\n",
            "Epoch: 1725, mean return: 201.25, mean episode length: 556.2\n",
            "Epoch: 1750, mean return: 186.03, mean episode length: 623.6\n",
            "Epoch: 1775, mean return: 178.20, mean episode length: 616.8\n",
            "Epoch: 1800, mean return: 209.67, mean episode length: 701.0\n",
            "Epoch: 1825, mean return: 112.63, mean episode length: 802.4\n",
            "Epoch: 1850, mean return: 127.98, mean episode length: 691.2\n",
            "Epoch: 1875, mean return: 228.42, mean episode length: 503.0\n",
            "Epoch: 1900, mean return: 127.45, mean episode length: 763.0\n",
            "Epoch: 1925, mean return: 255.98, mean episode length: 457.6\n",
            "Epoch: 1950, mean return: 178.76, mean episode length: 487.8\n",
            "Epoch: 1975, mean return: 118.66, mean episode length: 760.8\n",
            "Epoch: 2000, mean return: 154.75, mean episode length: 585.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(alg.policy, \"SAC_LunarLander_policy\")\n",
        "torch.save(alg.q_net1, \"SAC_LunarLander_qnet1\")\n",
        "torch.save(alg.q_net2, \"SAC_LunarLander_qnet2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vweaP3DPtP4U",
        "outputId": "49b087ec-d293-4b2b-9efb-5db572dd93e3"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "KmlGOe6KtA7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = torch.load(\"SAC_LunarLander_policy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4USZDtMtvPc",
        "outputId": "5e2c505f-0b0a-4122-a09a-cd2717934a56"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "def create_test_gym_environment(name):\n",
        "  environment = gym.make(name, render_mode=\"rgb_array\")\n",
        "  environment = RecordVideo(environment, video_folder=f\"./test_{name}\", episode_trigger=lambda x: x % 1 == 0)\n",
        "\n",
        "  return environment"
      ],
      "metadata": {
        "id": "lAfPxQ2MoOiu"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def play_episode(test_env, policy, repeat_action_rate):\n",
        "    state = test_env.reset()[0]\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        state_v = torch.tensor(state)\n",
        "        action, _ = policy(state_v)\n",
        "        action = action.cpu().numpy()\n",
        "\n",
        "        for _ in range(repeat_action_rate):\n",
        "          next_state, reward, done1, done2, info = test_env.step(action)\n",
        "        done = done1 or done2\n",
        "        state = next_state"
      ],
      "metadata": {
        "id": "FgH__yNpo7jN"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_env = create_test_gym_environment('LunarLanderContinuous-v2')\n",
        "\n",
        "for _ in range(10):\n",
        "  play_episode(test_env, policy, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK6r0wF_X3KJ",
        "outputId": "34911cda-0bb8-4b62-b6a4-ae6324149e56"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/record_video.py:87: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/test_LunarLanderContinuous-v2 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/test_LunarLanderContinuous-v2/rl-video-episode-0.mp4.\n",
            "Moviepy - Writing video /content/test_LunarLanderContinuous-v2/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/test_LunarLanderContinuous-v2/rl-video-episode-0.mp4\n",
            "Moviepy - Building video /content/test_LunarLanderContinuous-v2/rl-video-episode-1.mp4.\n",
            "Moviepy - Writing video /content/test_LunarLanderContinuous-v2/rl-video-episode-1.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/test_LunarLanderContinuous-v2/rl-video-episode-1.mp4\n",
            "Moviepy - Building video /content/test_LunarLanderContinuous-v2/rl-video-episode-2.mp4.\n",
            "Moviepy - Writing video /content/test_LunarLanderContinuous-v2/rl-video-episode-2.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/test_LunarLanderContinuous-v2/rl-video-episode-2.mp4\n",
            "Moviepy - Building video /content/test_LunarLanderContinuous-v2/rl-video-episode-3.mp4.\n",
            "Moviepy - Writing video /content/test_LunarLanderContinuous-v2/rl-video-episode-3.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/test_LunarLanderContinuous-v2/rl-video-episode-3.mp4\n",
            "Moviepy - Building video /content/test_LunarLanderContinuous-v2/rl-video-episode-4.mp4.\n",
            "Moviepy - Writing video /content/test_LunarLanderContinuous-v2/rl-video-episode-4.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/test_LunarLanderContinuous-v2/rl-video-episode-4.mp4\n",
            "Moviepy - Building video /content/test_LunarLanderContinuous-v2/rl-video-episode-5.mp4.\n",
            "Moviepy - Writing video /content/test_LunarLanderContinuous-v2/rl-video-episode-5.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/test_LunarLanderContinuous-v2/rl-video-episode-5.mp4\n",
            "Moviepy - Building video /content/test_LunarLanderContinuous-v2/rl-video-episode-6.mp4.\n",
            "Moviepy - Writing video /content/test_LunarLanderContinuous-v2/rl-video-episode-6.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/test_LunarLanderContinuous-v2/rl-video-episode-6.mp4\n",
            "Moviepy - Building video /content/test_LunarLanderContinuous-v2/rl-video-episode-7.mp4.\n",
            "Moviepy - Writing video /content/test_LunarLanderContinuous-v2/rl-video-episode-7.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/test_LunarLanderContinuous-v2/rl-video-episode-7.mp4\n",
            "Moviepy - Building video /content/test_LunarLanderContinuous-v2/rl-video-episode-8.mp4.\n",
            "Moviepy - Writing video /content/test_LunarLanderContinuous-v2/rl-video-episode-8.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/test_LunarLanderContinuous-v2/rl-video-episode-8.mp4\n",
            "Moviepy - Building video /content/test_LunarLanderContinuous-v2/rl-video-episode-9.mp4.\n",
            "Moviepy - Writing video /content/test_LunarLanderContinuous-v2/rl-video-episode-9.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/test_LunarLanderContinuous-v2/rl-video-episode-9.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E_iW20SqYGQQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSvndPmhjLzZ"
      },
      "source": [
        "# Double DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ0mmBnSjGFK"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6y2QJ3EkPBv"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium\n",
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5earqq-_kadS",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\", obs_type=\"ram\")\n",
        "env.reset()\n",
        "\n",
        "plt.imshow(env.render())\n",
        "plt.axis(\"off\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4qnwax3jJLf"
      },
      "source": [
        "## Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut3m8m7-bMoV"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Sequential, Linear, ReLU, Module, Conv2d, Flatten\n",
        "\n",
        "class QNet(Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, n_actions):\n",
        "    super().__init__()\n",
        "    self.net = Sequential(Linear(obs_size, hidden_size),\n",
        "                          ReLU(),\n",
        "                          Linear(hidden_size, hidden_size),\n",
        "                          ReLU(),\n",
        "                          Linear(hidden_size, hidden_size),\n",
        "                          ReLU(),\n",
        "                          Linear(hidden_size, n_actions))\n",
        "\n",
        "  def forward(self, state):\n",
        "      return self.net(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3uCrRDalrfp"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def epsilon_greedy(state, env, net, epsilon=0.0):\n",
        "  if np.random.random() < epsilon:\n",
        "    action = env.action_space.sample()\n",
        "  else:\n",
        "    q_values = net(state)\n",
        "    _, action = torch.max(q_values, dim=1)\n",
        "    action = int(action.item())\n",
        "  return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6Y0m2fFmIRv"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSeD5LQhmQsf"
      },
      "outputs": [],
      "source": [
        "from gymnasium.wrappers import RecordVideo, TimeLimit\n",
        "\n",
        "def create_gym_environment(name):\n",
        "  environment = gym.make(name, render_mode=\"rgb_array\", obs_type=\"ram\")\n",
        "  environment = RecordVideo(environment,\n",
        "                            video_folder=f'./{name}_recored_episodes',\n",
        "                            episode_trigger=lambda x: x % 25 == 0,\n",
        "                            disable_logger=True)\n",
        "\n",
        "  return environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pf6Cej3njB1"
      },
      "outputs": [],
      "source": [
        "def polyak_average(net, target_net, tau=0.01):\n",
        "  for qp, tp in zip(net.parameters(), target_net.parameters()):\n",
        "    tp.data.copy_(tau * qp.data + (1 - tau) * tp.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3fVmCJ1mWPh"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from copy import deepcopy\n",
        "import itertools\n",
        "from torch.nn.functional import smooth_l1_loss\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class DoubleDQN():\n",
        "\n",
        "  def __init__(self, env_name, nets=None, policy=epsilon_greedy, capacity=10_000,\n",
        "               batch_size=256, lr=1e-3, hidden_size=128, gamma=0.99, tau=0.05,\n",
        "               loss_fn=smooth_l1_loss, optim=AdamW, eps_start=1.0, eps_end=0.15,\n",
        "               eps_last_episode=600, samples_per_epoch=1024):\n",
        "\n",
        "      self.env = create_gym_environment(env_name)\n",
        "\n",
        "      obs_size = self.env.observation_space.shape[0]\n",
        "      n_actions = self.env.action_space.n\n",
        "\n",
        "      self.q_net1 = QNet(hidden_size, obs_size, n_actions).to(device) if not nets else nets[0].to(device)\n",
        "      self.q_net2 = QNet(hidden_size, obs_size, n_actions).to(device) if not nets else nets[1].to(device)\n",
        "\n",
        "      self.target_q_net1 = deepcopy(self.q_net1)\n",
        "      self.target_q_net2 = deepcopy(self.q_net2)\n",
        "\n",
        "      q_net_params = itertools.chain(self.q_net1.parameters(), self.q_net2.parameters())\n",
        "      self.optim = optim(q_net_params, lr=lr)\n",
        "\n",
        "      self.policy = policy\n",
        "      self.buffer = ReplayBuffer(capacity=capacity)\n",
        "      self.loss_fn = loss_fn\n",
        "\n",
        "      self.gamma = gamma\n",
        "      self.tau = tau\n",
        "      self.batch_size = batch_size\n",
        "      self.eps_start = eps_start\n",
        "      self.eps_end = eps_end\n",
        "      self.eps_last_episode = eps_last_episode\n",
        "      self.samples_per_epoch = samples_per_epoch\n",
        "      self.lr = lr\n",
        "\n",
        "      self.current_epoch = 1\n",
        "      self.log = []\n",
        "      self.returns = []\n",
        "      self.episode_lengths = []\n",
        "\n",
        "      while len(self.buffer) < self.samples_per_epoch:\n",
        "        self.play_episode(epsilon=1.0)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def play_episode(self, policy=None, epsilon=0.):\n",
        "      state = self.env.reset()[0]\n",
        "      state = torch.tensor(state)/255.0\n",
        "\n",
        "      done = False\n",
        "      rewards = 0\n",
        "      episode_length = 0\n",
        "\n",
        "      while not done:\n",
        "        if policy:\n",
        "          action = policy(state.unsqueeze(0), self.env, self.q_net1, epsilon=epsilon)\n",
        "        else:\n",
        "          action = self.env.action_space.sample()\n",
        "\n",
        "        next_state, reward, done1, done2, info = self.env.step(action)\n",
        "        done = done1 or done2\n",
        "        rewards += reward\n",
        "        episode_length += 1\n",
        "\n",
        "        next_state = torch.tensor(next_state, device=device)/255.0\n",
        "        action, reward, done = list(map(lambda x: torch.tensor(x, device=device), (action, reward, done)))\n",
        "\n",
        "        exp = (state, action, reward, done, next_state)\n",
        "        self.buffer.append(exp)\n",
        "        state = next_state\n",
        "      return rewards, episode_length\n",
        "\n",
        "  def fit(self, n_epoch):\n",
        "      for epoch in range(n_epoch):\n",
        "        loss_total = 0\n",
        "        for _ in range(self.samples_per_epoch//self.batch_size):\n",
        "          loss = self.training_step()\n",
        "          loss_total += loss\n",
        "\n",
        "        last_return, episode_length  = self.training_epoch_end()\n",
        "        self.returns.append(last_return)\n",
        "        self.episode_lengths.append(episode_length)\n",
        "        self.log.append([self.current_epoch, last_return, loss_total.item()])\n",
        "\n",
        "        if self.current_epoch % 25 == 0:\n",
        "          print(f\"Epoch: {self.current_epoch}, mean return: {np.mean(self.returns[-10:]):.2f}, \" \\\n",
        "           f\"mean episode length: {np.mean(self.episode_lengths[-10:])}, loss: {loss_total:.2f}\")\n",
        "\n",
        "  def training_step(self):\n",
        "      batch_T = self.buffer.sample(self.batch_size)\n",
        "      batch = list(map(torch.stack, zip(*batch_T)))\n",
        "\n",
        "      states, actions, rewards, dones, next_states = batch\n",
        "      actions = actions.unsqueeze(1)\n",
        "      rewards = rewards.unsqueeze(1)\n",
        "      dones = dones.unsqueeze(1)\n",
        "\n",
        "      state_action_values1 = self.q_net1(states).gather(1, actions)\n",
        "      state_action_values2 = self.q_net2(states).gather(1, actions)\n",
        "\n",
        "      next_action_values1, _ = self.target_q_net1(next_states).max(dim=1, keepdim=True)\n",
        "      next_action_values2, _ = self.target_q_net2(next_states).max(dim=1, keepdim=True)\n",
        "      next_action_values = torch.min(next_action_values1, next_action_values2)\n",
        "\n",
        "      next_action_values = next_action_values1\n",
        "      next_action_values[dones] = 0.0\n",
        "\n",
        "      expected_state_action_values = rewards + self.gamma * next_action_values\n",
        "\n",
        "      loss1 = self.loss_fn(state_action_values1, expected_state_action_values)\n",
        "      loss2 = self.loss_fn(state_action_values2, expected_state_action_values)\n",
        "      loss = loss1 + loss2\n",
        "\n",
        "      self.optim.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optim.step()\n",
        "\n",
        "      return loss\n",
        "\n",
        "  def training_epoch_end(self):\n",
        "      epsilon = max(self.eps_end, self.eps_start - self.current_epoch / self.eps_last_episode)\n",
        "\n",
        "      last_return, episode_length = self.play_episode(policy=self.policy, epsilon=epsilon)\n",
        "\n",
        "      polyak_average(self.q_net1, self.target_q_net1, tau=self.tau)\n",
        "      polyak_average(self.q_net2, self.target_q_net2, tau=self.tau)\n",
        "\n",
        "      self.current_epoch += 1\n",
        "      return last_return, episode_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlzxV6BobMoX"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvR4kPoN53Fw",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "algo_double_dqn = DoubleDQN(\"ALE/MsPacman-v5\")\n",
        "algo_double_dqn.fit(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "LkSRQ7MhbMoX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(algo_double_dqn.log, columns=[\"epoch\", \"return\", \"loss\"])\n",
        "df.plot(\"epoch\", \"return\", kind=\"scatter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tB6JRYKkbMoY"
      },
      "outputs": [],
      "source": [
        "net1 = algo_double_dqn.q_net1\n",
        "net2 = algo_double_dqn.q_net2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zqa0ARdvbMoY"
      },
      "outputs": [],
      "source": [
        "torch.save(net1, \"pacman_qnet1\")\n",
        "torch.save(net2, \"pacman_qnet2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XplL-rF8bMoY"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PB7yr36bMoY"
      },
      "outputs": [],
      "source": [
        "q_net1 = torch.load(\"pacman_qnet1\")\n",
        "q_net2 = torch.load(\"pacman_qnet2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrd655qybMoY"
      },
      "outputs": [],
      "source": [
        "from gymnasium.wrappers import RecordVideo, TimeLimit\n",
        "\n",
        "def create_test_gym_environment():\n",
        "  environment = gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\", obs_type=\"ram\")\n",
        "  environment = TimeLimit(environment, max_episode_steps=800)\n",
        "  environment = RecordVideo(environment, video_folder=f'./test_MsPacman-v5_recored_episodes', episode_trigger=lambda x: x % 1 == 0, disable_logger=True)\n",
        "\n",
        "  return environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "eN4QyPljbMoZ"
      },
      "outputs": [],
      "source": [
        "env = create_test_gym_environment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QF40qV8bMoZ"
      },
      "outputs": [],
      "source": [
        "  @torch.no_grad()\n",
        "  def play_episode(env, net):\n",
        "      state = env.reset()[0]\n",
        "      state = torch.tensor(state)/255.0\n",
        "\n",
        "      done = False\n",
        "      rewards = 0\n",
        "      episode_length = 0\n",
        "\n",
        "      while not done:\n",
        "        action = epsilon_greedy(state.unsqueeze(0), env, net, epsilon=0.0)\n",
        "        if random.random() < 0.05:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        next_state, reward, done1, done2, _ = env.step(action)\n",
        "        done = done1 or done2\n",
        "        rewards += reward\n",
        "        episode_length += 1\n",
        "\n",
        "        next_state = torch.tensor(next_state)/255.0\n",
        "        state = next_state\n",
        "      return rewards, episode_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rl8PkumKbMoZ"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    rew, el = play_episode(env, q_net2)\n",
        "    print(f\"i: {i}, Reward: {rew}, episode length: {el}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHg_ZrG-bMoZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}